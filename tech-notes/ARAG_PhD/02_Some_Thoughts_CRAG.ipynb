{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Thoughts and Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attached: CRAG xarchive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請以markdown心智圖筆記的概念來為我整理這份pdf文件，從文件目錄結構作為第一層，向下深入整理出重要的知識點、研究方法、研究成果、實驗數據洞察、結論，輸出包括階層式標題，筆記內容以及可以豐富視覺化的表情符號。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是根據你的 PDF 論文《Corrective Retrieval Augmented Generation》所整理的 Markdown 心智圖風格筆記，從目錄結構出發，逐層展開包含關鍵知識點、研究方法、實驗洞察與結論，並搭配表情符號以豐富視覺效果。你可以直接貼到 Markdown 支援的筆記軟體中（如 Obsidian、Notion、Typora）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是根據你的 PDF 論文《Corrective Retrieval Augmented Generation》所整理的 Markdown 心智圖風格筆記，從目錄結構出發，逐層展開包含關鍵知識點、研究方法、實驗洞察與結論，並搭配表情符號以豐富視覺效果。你可以直接貼到 Markdown 支援的筆記軟體中（如 Obsidian、Notion、Typora）。\n",
    "\n",
    "---\n",
    "\n",
    "# 📚 Corrective Retrieval Augmented Generation (CRAG) 論文筆記\n",
    "\n",
    "> 一個增強 RAG 韌性的新方法：自我糾錯 + 網路搜尋 + 精煉知識 🔍🤖🌐\n",
    "\n",
    "## 🧭 1. Introduction\n",
    "- LLMs 具備強大語言生成能力，但仍常出現幻覺 ❗\n",
    "- RAG 作為解法，卻高度依賴檢索品質\n",
    "- 問題：當檢索結果錯誤，RAG 也會產生錯誤生成\n",
    "- 提出：**CRAG** 💡\n",
    "  - 加入評估器 → 判斷檢索結果品質\n",
    "  - 低信心 → 啟動網路補充搜尋\n",
    "  - 加入知識精煉 → 濾掉無關片段\n",
    "- 🔌 Plug-and-play，適用多種 RAG 方法\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 2. Related Work\n",
    "### 🧠 2.1 Hallucination in LLMs\n",
    "- 錯誤知識激活是常見原因\n",
    "- 訓練數據品質、分布、更新時間不一致導致幻覺\n",
    "\n",
    "### 📦 2.2 Retrieval-Augmented Generation (RAG)\n",
    "- 使用檢索補充 LLM 知識\n",
    "- 弱點：未檢查檢索準確性，可能傳遞錯誤資訊\n",
    "\n",
    "### 🔧 2.3 Advanced RAG Methods\n",
    "- Self-RAG：判斷是否需要檢索\n",
    "- Toolformer, SAIL：強化檢索使用時機與方式\n",
    "- 然而：少有方法處理「錯誤檢索」的情境\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 3. Task Formulation\n",
    "- 任務：根據輸入 X 與知識庫 C，生成輸出 Y\n",
    "- 分為：\n",
    "  - 📥 Retriever：找出前 K 篇相關文檔 D\n",
    "  - 📤 Generator：根據 D + X 產生 Y\n",
    "- 公式：\n",
    "  ```math\n",
    "  P(Y|X) = P(D|X) * P(Y|D,X)\n",
    "  ```\n",
    "- 關鍵問題：檢索錯誤時，生成將失敗\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ 4. CRAG 方法概覽\n",
    "\n",
    "### 📈 4.1 總體架構（Inference Flow）\n",
    "- 啟用輕量化的 Retrieval Evaluator（T5-Large）\n",
    "- 評估檢索結果相關性 → 觸發三種行動：\n",
    "  - ✅ Correct\n",
    "  - ❌ Incorrect\n",
    "  - ⚖️ Ambiguous\n",
    "- 行動決策邏輯：\n",
    "  - Correct → 內部知識精煉\n",
    "  - Incorrect → 啟用網路搜尋 + ChatGPT 重寫搜尋詞\n",
    "  - Ambiguous → 兩者結合\n",
    "\n",
    "### 📊 4.2 Retrieval Evaluator\n",
    "- 模型：Fine-tuned T5-Large 🧠\n",
    "- 任務：預測 Question-Document 配對是否相關\n",
    "- 訓練數據：PopQA 的標註與負樣本對比組\n",
    "\n",
    "### 🚦 4.3 Action Trigger\n",
    "- 設定信心閾值，高於則為 Correct，低於為 Incorrect，其餘為 Ambiguous\n",
    "- 每一份文檔分別評估後彙總\n",
    "\n",
    "### 🧹 4.4 Knowledge Refinement\n",
    "- ✂️ 分段處理文檔為 strips\n",
    "- 🧪 各 strip 經由 evaluator 評估\n",
    "- ✅ 留下相關的 strips → 重組為乾淨知識\n",
    "\n",
    "### 🌐 4.5 Web Search 補充機制\n",
    "- 使用 ChatGPT 重寫查詢詞為關鍵詞\n",
    "- 使用 Web Search API（如 Google）\n",
    "- 優先篩選 Wikipedia 等可信來源\n",
    "- 同樣經過精煉流程處理\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 5. 實驗設計與結果\n",
    "\n",
    "### 📚 5.1 資料集與任務\n",
    "| 資料集        | 任務類型           | 評估指標     |\n",
    "|---------------|--------------------|--------------|\n",
    "| PopQA         | 短句問答           | Accuracy     |\n",
    "| Biography     | 長文本生成         | FactScore    |\n",
    "| PubHealth     | 真偽判定           | Accuracy     |\n",
    "| Arc-Challenge | 常識選擇題         | Accuracy     |\n",
    "\n",
    "### 🧪 5.2 實驗結果亮點\n",
    "\n",
    "**CRAG 在各項任務皆顯著超越 baseline** 💥：\n",
    "\n",
    "| 模型          | PopQA | Biography | PubHealth | ARC |\n",
    "|---------------|--------|------------|------------|-----|\n",
    "| RAG           | 50.5   | 44.9       | 48.9       | 43.4 |\n",
    "| CRAG 🆚        | **54.9** | **47.7**     | **59.5**     | **53.7** |\n",
    "| Self-RAG      | 54.9   | 81.2       | 72.4       | 67.3 |\n",
    "| Self-CRAG 🔥   | **61.8** | **86.2**     | **74.8**     | **67.2** |\n",
    "\n",
    "### 🧪 5.4 消融實驗（Ablation Study）\n",
    "\n",
    "每一個模塊貢獻顯著，去除後性能下降 📉：\n",
    "\n",
    "| 移除項目            | PopQA (Acc) |\n",
    "|---------------------|-------------|\n",
    "| w/o Correct         | -1.7%       |\n",
    "| w/o Incorrect       | -0.5%       |\n",
    "| w/o Ambiguous       | -0.9%       |\n",
    "| w/o Knowledge Refinement | -5.1%  |\n",
    "| w/o Query Rewriting | -3.2%       |\n",
    "| w/o Knowledge Selection | -4.0%   |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 6. 結論與未來工作\n",
    "\n",
    "### 🎯 結論：\n",
    "- CRAG 能顯著提升 RAG 韌性與效能\n",
    "- 精練補充知識 → 抵抗幻覺生成\n",
    "- 輕量評估器 + 模組化設計 → 易於整合\n",
    "\n",
    "### 🔮 限制與展望：\n",
    "- 須額外訓練 retrieval evaluator\n",
    "- 未來希望讓 LLM 內建評估能力（end-to-end）\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 附錄連結\n",
    "- 🧬 [CRAG GitHub Repo](https://github.com/HuskyInSalt/CRAG)\n",
    "- 📄 原始論文：Yan et al., 2024\n",
    "\n",
    "---\n",
    "\n",
    "需要我幫你把這份筆記轉成可視化心智圖（如 mermaid 或畫圖工具）嗎？🧠🗺️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你多次提到評估器，是像Trulens那樣的外掛，還是隨便寫一個？難道是呼叫openAI或者T5模型讓它打分嗎？你能詳細說一下，這個評估器有多大效果，怎麼implement？最重要的重點是評估器還是大模型的質量？未來我要在CRAG上面讀博士的話，如何發想我的ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是一個非常關鍵的問題，從你提的角度切入非常準。💡CRAG 的核心創新就在於它引入了「Retrieval Evaluator（檢索評估器）」作為自我糾錯的中樞。讓我們一步步拆解它的本質、實作、效果與未來潛力。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 1. CRAG 的 Retrieval Evaluator 是什麼？\n",
    "\n",
    "**它不是像 TruLens 那樣的外掛或監控工具，也不是簡單地打個分數，而是一個經過專門訓練的小型模型，來**：\n",
    "\n",
    "> **判斷「一段檢索結果」是否真能幫助回答某個問題。**\n",
    "\n",
    "- 📌 模型架構：使用的是 **T5-Large**（770M 參數），不是 GPT 或 ChatGPT。\n",
    "- 📚 資料：使用了 PopQA 的標註數據來訓練\n",
    "- 🔍 類似「文本對」任務：輸入是 `(question, document)` 對，輸出是是否相關（是/否）\n",
    "\n",
    "這種設計讓它成為一個 lightweight 的 **二分類模型**，訓練成本小，推理快速，**不依賴 GPT API**，且可嵌入系統中自動運行。🚀\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 2. 效果好嗎？對整體貢獻有多大？\n",
    "\n",
    "非常關鍵！作者在 **表 4** 做了對比實驗 👇：\n",
    "\n",
    "| 方法                       | 評估器準確率 |\n",
    "|----------------------------|---------------|\n",
    "| 自己訓練的 T5 評估器       | **84.3%** ✅ |\n",
    "| ChatGPT（zero-shot）      | 58.0% ❌ |\n",
    "| ChatGPT + CoT 推理        | 62.4% |\n",
    "| ChatGPT + few-shot 示例   | 64.7% |\n",
    "\n",
    "📣 結論：這個 T5 評估器，不僅比商用 ChatGPT 更精準，還省錢省時，適合在部署中使用。\n",
    "\n",
    "而且在消融實驗中顯示，如果移除這個評估器，CRAG 整體效果大幅下降（如 PopQA 降低 5.1% 準確率）。因此：\n",
    "> **這個評估器是整個 CRAG 的核心部件之一，幫助「讓 RAG 不再盲信錯誤檢索內容」。**\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ 3. 如何實作 Retrieval Evaluator？\n",
    "\n",
    "### 🧱 模型：\n",
    "- 使用 HuggingFace 的 `google/t5-large`\n",
    "- fine-tune 用於 binary classification（是否 relevant）\n",
    "\n",
    "### 🧪 資料來源：\n",
    "- **正樣本**：如 PopQA 中的 golden document 與問題配對\n",
    "- **負樣本**：從 Contriever 檢索回來的、與 query 看似相關但實際無關的段落\n",
    "\n",
    "### 🏗️ 訓練步驟：\n",
    "```python\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "\n",
    "# Input: \"question: What is ... document: [passage]\"\n",
    "# Target: \"yes\" or \"no\"\n",
    "\n",
    "# 自行構建 Dataset，訓練一個生成 \"yes\"/\"no\" 的 T5\n",
    "```\n",
    "\n",
    "也可改為分類模型，例如使用 BERT + classifier head 做 binary classification，更快更省。\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 4. 博士研究方向建議：如何基於 CRAG 發展新的 ideas？\n",
    "\n",
    "這是最有趣的部分。如果你想在 CRAG 上讀博士，可以從下列方向發想 🔭：\n",
    "\n",
    "### 🌱（1）自監督訓練 Retrieval Evaluator 🧠\n",
    "> 目前需要標註資料（如 PopQA），但能否「無標註」訓練？\n",
    "\n",
    "- 探索 **contrastive learning**（類似 SimCSE）來構建 relevance score\n",
    "- 讓系統根據生成輸出與 gold answer 的一致性自我標註\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️（2）將 Retrieval Evaluator 融入 LLM 內部（End-to-End）🧬\n",
    "> 評估器目前是額外模塊，能否內嵌於 LLM，實現 joint retrieval filtering？\n",
    "\n",
    "- 設計 multi-task LLM：輸入 query + docs，輸出回答 + score（或信心）\n",
    "- 或透過 **reinforcement learning** 教會 LLM 自我選擇資料源\n",
    "\n",
    "---\n",
    "\n",
    "### 🕸️（3）多模態檢索評估器 🌐🖼️\n",
    "> 文件若含圖片、表格或圖譜，如何處理？\n",
    "\n",
    "- 研究多模態（text + vision）評估器\n",
    "- 或應用在醫學問答、科學知識查詢中\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪（4）Uncertainty-aware Action Trigger 🎯\n",
    "> 現在用的是硬閾值判斷，是否能引入更細膩的不確定性建模？\n",
    "\n",
    "- 如：Bayesian confidence estimation、MC Dropout\n",
    "- 或學習 soft decision weights，而不是 3 類 one-hot 判斷\n",
    "\n",
    "---\n",
    "\n",
    "### 🔌（5）改進 knowledge refinement 模塊 ✂️\n",
    "> strip 分段 + 打分 + 重組，目前是 heuristic，如果用學習式更好？\n",
    "\n",
    "- 探索 text segmentation + ranking 模型自動挑關鍵片段\n",
    "- 或學習 strip importance 分數\n",
    "\n",
    "---\n",
    "\n",
    "## 💭 總結：CRAG 的靈魂在於「選資料」而不是「寫資料」\n",
    "\n",
    "🔑 大模型的能力已經很強了，但如果資料是錯的，它也救不了。因此：\n",
    "\n",
    "> CRAG 的重點是讓系統「知道哪段資料不能用、應該丟」，而 Retrieval Evaluator 正是這個決策的心臟 💗。\n",
    "\n",
    "你的研究如果能讓 LLM 更自覺地使用知識，而不是盲信資料，那就是走在前沿。\n",
    "\n",
    "---\n",
    "\n",
    "想不想我幫你草擬一份博士 proposal 提綱來發展 CRAG 的未來研究？📄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是的，這個**評估器**（Retrieval Evaluator）在 CRAG 中是一個非常關鍵的組件，其主要作用是用來判斷檢索的文檔是否與輸入問題相關，並基於這些評估結果決定後續的處理步驟。評估器並不是像 Trulens 這樣的外掛，而是 CRAG 中的一個核心模組，它被設計為輕量級模型，並且可以簡單集成到現有的 RAG（Retrieval-augmented Generation）框架中。\n",
    "\n",
    "### 1. **評估器的功能和作用**\n",
    "\n",
    "在 CRAG 中，評估器的作用是：\n",
    "- **評估檢索結果的相關性**：對於從知識庫中檢索回來的文檔，評估器會判斷這些文檔是否能有效回答給定問題。\n",
    "- **生成信心分數**：它會為每個檢索文檔生成一個信心分數，然後根據這些分數觸發不同的行動（Correct, Incorrect, Ambiguous）。\n",
    "  - **Correct**：如果文檔內容是準確的，則會進行內部知識精煉。\n",
    "  - **Incorrect**：如果文檔完全不相關，則會丟棄這些文檔，並進行網絡搜尋補充。\n",
    "  - **Ambiguous**：如果難以判定文檔的準確性，則會進行折衷處理，結合內部和外部知識來生成答案。\n",
    "\n",
    "### 2. **如何實現這個評估器**\n",
    "\n",
    "在 CRAG 中，這個評估器實際上是基於一個較小的語言模型，具體來說是**T5**（Text-to-Text Transfer Transformer）。這個評估器並不是直接用大型模型（如 GPT-3 或 GPT-4）來進行推理，而是使用經過微調的輕量 T5 模型來判斷檢索結果的質量，這樣能夠在不大幅增加計算開銷的情況下進行高效的評估。\n",
    "\n",
    "**實現步驟**：\n",
    "1. **微調 T5 模型**：使用一組標註過的數據來微調 T5，使其能夠根據檢索文檔和問題的配對來預測相關性。例如，這些數據集可能來自 PopQA，其中包含了問題和與其相關的、部分不相關的文檔。\n",
    "2. **評估過程**：對每個檢索結果，將問題和文檔對進行合併並傳遞給評估器。模型會返回一個相關性得分，根據這個得分來決定文檔是否應該被認為是準確的。\n",
    "3. **基於得分觸發行動**：當信心得分超過一定閾值，則認為文檔是正確的；當信心得分過低時，則認為文檔不相關，並進行替代處理。\n",
    "\n",
    "### 3. **評估器的效果**\n",
    "\n",
    "- **性能提升**：CRAG 在與傳統 RAG 相比時，顯示了顯著的性能提升。特別是在 PopQA、Biography、PubHealth 和 Arc-Challenge 等多個數據集上，CRAG 能夠有效提高檢索結果的準確性和生成質量。這表明，評估器對檢索結果的準確性進行評估是大幅提升系統整體表現的關鍵。\n",
    "- **相對重要性**：評估器在 CRAG 中扮演了至關重要的角色，因為如果檢索結果不準確，無論後續生成器多麼強大，都無法提供正確答案。這使得評估器的準確性和效能對整體系統的效果起到了核心作用。\n",
    "\n",
    "### 4. **如何提升評估器的效果**\n",
    "\n",
    "- **更大模型的質量**：雖然現在評估器是基於 T5 模型實現的，未來可以考慮使用更大、更精細的模型（如 GPT-4 或其他高效的 LLMs），這樣可以提高對檢索結果的判斷準確度，進一步提升 CRAG 整體系統的性能。\n",
    "- **強化微調技術**：除了使用標註數據外，還可以利用更多實際應用場景中的數據來進行微調，使評估器對各種問題和領域的理解更為深入。\n",
    "\n",
    "### 5. **如何在 CRAG 上發想博士研究的 ideas**\n",
    "\n",
    "如果你打算在 CRAG 上進行博士研究，以下是幾個可能的研究方向：\n",
    "\n",
    "1. **增強評估器的能力**：\n",
    "   - 可以探索如何讓評估器變得更加智能，能夠自動識別文檔中的虛假或不一致信息，而不僅僅是依賴簡單的相關性評估。\n",
    "   - 比如，設計一種更高效的動態微調方法，使得評估器能夠根據新的問題類型或領域進行自適應調整。\n",
    "\n",
    "2. **擴展知識庫與外部搜尋的集成**：\n",
    "   - 目前的 CRAG 方法主要通過 Web 搜索來補充外部知識，未來可以研究如何智能選擇更優質的外部知識來源，甚至是實時更新的動態知識庫。\n",
    "   - 你也可以探索如何將更多多模態知識（如圖片、圖表、視頻）融入 RAG 模型中。\n",
    "\n",
    "3. **多任務學習與通用性提升**：\n",
    "   - 目前 CRAG 在多個任務上表現良好，但仍有可能進一步擴展其通用性。你可以探索如何讓 CRAG 系統能夠處理更多類型的生成任務（如文本摘要、翻譯等）。\n",
    "   - 多任務學習也是一個重要方向，通過在多個任務間共享知識來進一步提高模型的效能。\n",
    "\n",
    "4. **系統效率與可擴展性**：\n",
    "   - 雖然 CRAG 已經在效率上取得一定成就，但仍有優化空間。你可以探索如何使 CRAG 在大規模應用中保持高效，如通過硬體加速、分布式處理等手段來提升性能。\n",
    "\n",
    "總結來說，CRAG 的成功取決於對檢索結果的高效評估，這使得未來在這個領域的研究可以朝向提升模型的評估能力、擴展知識庫使用範圍、並提升多任務的適應性和效率等方向發展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
